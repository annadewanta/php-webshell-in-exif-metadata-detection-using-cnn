{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 02: Pelatihan dan Evaluasi Model\n",
        "\n",
        "Notebook ini mencakup tiga proses utama dalam pembangunan model:\n",
        "1.  **Hyperparameter Tuning:** Mencari arsitektur CNN dan konfigurasi pelatihan terbaik menggunakan Keras Tuner dengan strategi Bayesian Optimization.\n",
        "2.  **Pelatihan Final:** Melatih model dengan konfigurasi terbaik pada gabungan data latih dan validasi.\n",
        "3.  **Evaluasi:** Mengukur kinerja model final pada data uji yang independen."
      ],
      "metadata": {
        "id": "BMfrdNt8Upb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGqWoLTvUm6p"
      },
      "outputs": [],
      "source": [
        "# --- Instalasi Pustaka ---\n",
        "!pip install keras-tuner --quiet\n",
        "\n",
        "# --- Hubungkan ke Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Impor Pustaka Utama ---\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras import layers, callbacks\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "import seaborn as sns\n",
        "import sys\n",
        "\n",
        "# --- Inisialisasi ---\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 1: Konfigurasi dan Pemuatan Data\n",
        "\n",
        "Pada langkah ini, kita akan mendefinisikan parameter-parameter utama untuk proses pelatihan dan memuat dataset `.npy` yang telah disiapkan sebelumnya."
      ],
      "metadata": {
        "id": "_0WSis4IUz0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- KONFIGURASI PELATIHAN ---\n",
        "# Pengguna akan diminta memasukkan path yang relevan.\n",
        "DATASET_DIR = input(\"Masukkan path ke direktori dataset (berisi file .npy): \")\n",
        "OUTPUT_DIR = input(\"Masukkan path ke direktori output (untuk menyimpan model dan hasil): \")\n",
        "\n",
        "# Parameter untuk proses tuning dan pelatihan\n",
        "MAX_TRIALS = 50\n",
        "EXECUTIONS_PER_TRIAL = 1\n",
        "TUNING_EPOCHS = 50\n",
        "FINAL_EPOCHS = 100\n",
        "PATIENCE = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# --- FUNGSI PEMUATAN DATA ---\n",
        "def load_data(preprocess_dir):\n",
        "    print(\"\\nMemuat dataset...\")\n",
        "    try:\n",
        "        X_train = np.load(os.path.join(preprocess_dir, \"X_train.npy\"))\n",
        "        y_train = np.load(os.path.join(preprocess_dir, \"y_train.npy\"))\n",
        "        X_val = np.load(os.path.join(preprocess_dir, \"X_val.npy\"))\n",
        "        y_val = np.load(os.path.join(preprocess_dir, \"y_val.npy\"))\n",
        "        X_test = np.load(os.path.join(preprocess_dir, \"X_test.npy\"))\n",
        "        y_test = np.load(os.path.join(preprocess_dir, \"y_test.npy\"))\n",
        "\n",
        "        print(\"\\nüìä Data Shapes:\")\n",
        "        print(f\"Train: {X_train.shape}, {y_train.shape}\")\n",
        "        print(f\"Val:   {X_val.shape}, {y_val.shape}\")\n",
        "        print(f\"Test:  {X_test.shape}, {y_test.shape}\\n\")\n",
        "        return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"‚ùå Error: File dataset tidak ditemukan. Pastikan path '{preprocess_dir}' benar.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# --- EKSEKUSI PEMUATAN DATA ---\n",
        "(X_train, y_train), (X_val, y_val), (X_test, y_test) = load_data(DATASET_DIR)"
      ],
      "metadata": {
        "id": "HLl_5I3AU0sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 2: Perancangan Arsitektur dan Hyperparameter Tuning\n",
        "\n",
        "Mendefinisikan ruang pencarian arsitektur CNN dan menjalankan proses tuning."
      ],
      "metadata": {
        "id": "kwA91KBnU3Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNGSI UNTUK MEMBANGUN MODEL (UNTUK TUNER) ---\n",
        "def build_model(hp):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Input(shape=(256, 256, 1)))\n",
        "    num_blocks = hp.Int('num_blocks', 2, 4)\n",
        "    for i in range(num_blocks):\n",
        "        kernel_size = hp.Choice(f'kernel_{i}', [3, 5, 7])\n",
        "        filters = hp.Choice(f'filters_{i}', [32, 64, 128])\n",
        "        model.add(layers.Conv2D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same'))\n",
        "        pool_size = hp.Choice(f'pool_{i}', [2, 3, 4])\n",
        "        model.add(layers.MaxPooling2D(pool_size))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        dropout_rate = hp.Float(f'dropout_{i}', 0.2, 0.5, step=0.1)\n",
        "        model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "    dense_units = hp.Int('dense_units', 64, 512, step=64)\n",
        "    model.add(layers.Dense(dense_units, activation='relu'))\n",
        "    model.add(layers.Dropout(hp.Float('dense_dropout', 0.3, 0.7, step=0.1)))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    optimizer_name = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
        "    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
        "    optimizer = tf.keras.optimizers.get(optimizer_name)\n",
        "    optimizer.learning_rate = lr\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# --- EKSEKUSI HYPERPARAMETER TUNING ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MEMULAI HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "overwrite_tuning = input(\"Mulai tuning baru (hapus hasil lama)? (true/false): \").lower() == 'true'\n",
        "\n",
        "tuner = kt.BayesianOptimization(\n",
        "    build_model,\n",
        "    objective='val_auc',\n",
        "    max_trials=MAX_TRIALS,\n",
        "    executions_per_trial=EXECUTIONS_PER_TRIAL,\n",
        "    directory=OUTPUT_DIR,\n",
        "    project_name='webshell_tuning',\n",
        "    overwrite=overwrite_tuning\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=TUNING_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nüíæ Menyimpan hasil tuning...\")\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_hp_path = os.path.join(OUTPUT_DIR, 'best_hyperparameters.json')\n",
        "with open(best_hp_path, 'w') as f:\n",
        "    json.dump(best_hps.values, f, indent=2)\n",
        "print(f\"‚úÖ Hyperparameter terbaik disimpan di: {best_hp_path}\")"
      ],
      "metadata": {
        "id": "giTVr0OmU4e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 3: Pelatihan Model Final\n",
        "\n",
        "Membangun model dengan arsitektur terbaik yang ditemukan dan melatihnya pada gabungan data latih dan validasi."
      ],
      "metadata": {
        "id": "gebr89NfU7Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EKSEKUSI PELATIHAN MODEL FINAL ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MEMULAI PELATIHAN MODEL FINAL\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "final_model = tuner.hypermodel.build(best_hps)\n",
        "print(\"Arsitektur Model Final:\")\n",
        "final_model.summary()\n",
        "\n",
        "X_full = np.concatenate((X_train, X_val))\n",
        "y_full = np.concatenate((y_train, y_val))\n",
        "\n",
        "output_model_path = os.path.join(OUTPUT_DIR, 'best_model.keras')\n",
        "final_callbacks = [\n",
        "    callbacks.EarlyStopping(monitor='loss', patience=PATIENCE, restore_best_weights=True),\n",
        "    callbacks.ModelCheckpoint(output_model_path, monitor='loss', save_best_only=True, mode='min'),\n",
        "    callbacks.CSVLogger(os.path.join(OUTPUT_DIR, 'final_training_log.csv')),\n",
        "    callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "]\n",
        "\n",
        "print(f\"\\nüöÄ Memulai training... Model akan disimpan di '{output_model_path}'\")\n",
        "history = final_model.fit(\n",
        "    X_full, y_full,\n",
        "    epochs=FINAL_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=final_callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "puOOsfhdU99-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 4: Evaluasi dan Analisis Hasil\n",
        "\n",
        "Mengevaluasi kinerja model final pada data uji dan memvisualisasikan hasilnya."
      ],
      "metadata": {
        "id": "Tx10lxdXU_5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EKSEKUSI EVALUASI ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MEMULAI EVALUASI MODEL\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "y_pred_probs = final_model.predict(X_test, batch_size=BATCH_SIZE)\n",
        "y_pred_classes = (y_pred_probs > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "print(\"\\nüìù Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes, target_names=['benign', 'malicious']))\n",
        "\n",
        "# --- Visualisasi Hasil ---\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['benign', 'malicious'], yticklabels=['benign', 'malicious'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'))\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'roc_curve.png'))\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Semua proses selesai!\")"
      ],
      "metadata": {
        "id": "lFkZmmBqVBR-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}